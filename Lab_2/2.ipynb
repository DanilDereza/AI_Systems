{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bbc3a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import pymorphy3\n",
    "import warnings\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33ed9b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Алекс Каменев\\n   Макс Вольф 2.\\n   Наемник.\\n   Глава 1.\\n   Станция технического обслуживания «Бринга-17», Лиманский Союз, приграничная территория Содруж'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/russian_stopwords.txt\", 'r', encoding='utf-8') as file:\n",
    "    stop_words = file.read().split()\n",
    "\n",
    "\n",
    "with open(\"data/Kamenev_Maks-Volf_2_Naemnik_RuLit_Me.txt\", 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "data[:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b61f0573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Алекс Каменев\\n   Макс Вольф \\n   Наемник.\\n   Глава \\n   Станция технического обслуживания «Бринга», Лиманский Союз, приграничная территория Содружества.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
    "regex = re.compile(url_pattern)\n",
    "data = regex.sub('', data)\n",
    "\n",
    "data = re.sub(r'\\b\\d[\\w\\-\\.]*', '', data)\n",
    "data = re.sub(r'\\b\\-|\\-\\b', '', data)\n",
    "data[:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2756df84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'алекс каменев макс вольф наемник. глава станция технического обслуживания «бринга», лиманский союз, приграничная территория содружества. то есть как, забло'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data.lower().strip().split())\n",
    "data[:155]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473e7f2",
   "metadata": {},
   "source": [
    "### <center>First method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecbda8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Лемматизация слов: 100%|███████████████████████████████████████████████████████| 108331/108331 [18:06<00:00, 99.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    infix_re = spacy.util.compile_infix_regex([pattern for pattern in nlp.Defaults.infixes if '-' not in pattern])\n",
    "\n",
    "    return Tokenizer(\n",
    "        nlp.vocab,\n",
    "        rules=nlp.Defaults.tokenizer_exceptions,\n",
    "        prefix_search=prefix_re.search,\n",
    "        suffix_search=suffix_re.search,\n",
    "        infix_finditer=infix_re.finditer,\n",
    "        token_match=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def lemmatize_word_list(text):\n",
    "    word_list = text.split()\n",
    "    return [\n",
    "        nlp(word)[0].lemma_\n",
    "        for word in tqdm(word_list, desc=\"Лемматизация слов\")\n",
    "        if not nlp(word)[0].is_stop and not nlp(word)[0].is_punct\n",
    "    ]\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "result1 = lemmatize_word_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f3c890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['алекс', 'каменев', 'макс', ..., 'дорасских', 'процент', 'акция'],\n",
       "      dtype='<U35')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac109e",
   "metadata": {},
   "source": [
    "### <center>Second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cdf6b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Лемматизация слов №2: 100%|█████████████████████████████████████████████████| 106024/106024 [00:08<00:00, 11868.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'\\b[\\w\\-]+\\b', text)\n",
    "\n",
    "\n",
    "def lemmatize_word_list2(text):\n",
    "    tokens = custom_tokenizer(text)\n",
    "    lemmatized = [\n",
    "        morph.parse(token)[0].normal_form\n",
    "        for token in tqdm(tokens, desc=\"Лемматизация слов №2\")\n",
    "        if token not in stop_words_list and re.search(r'\\w', token)\n",
    "    ]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "stop_words_list = list(set(stop_words + russian_stopwords))\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "result2 = lemmatize_word_list2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4694308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['алекс', 'каменеть', 'макс', ..., 'маркиз', 'дорасский', 'акция'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa8000",
   "metadata": {},
   "source": [
    "### <center>Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb96971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('человек', 369),\n",
       " ('время', 266),\n",
       " ('корабль', 254),\n",
       " ('сторона', 208),\n",
       " ('новый', 205),\n",
       " ('рука', 196),\n",
       " ('планета', 193),\n",
       " ('два', 191),\n",
       " ('сказать', 190),\n",
       " ('дело', 187)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('корабль', 255),\n",
       " ('человек', 242),\n",
       " ('планета', 215),\n",
       " ('сторона', 206),\n",
       " ('мой', 189),\n",
       " ('канваля', 189),\n",
       " ('рука', 180),\n",
       " ('новый', 170),\n",
       " ('баронство', 169),\n",
       " ('тисара', 166)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_BoW(tokens):\n",
    "    word_counts = {}\n",
    "    for token in tokens:\n",
    "        if token in word_counts:\n",
    "            word_counts[token] += 1\n",
    "        else:\n",
    "            word_counts[token] = 1\n",
    "\n",
    "    sorted_word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return sorted_word_counts\n",
    "\n",
    "\n",
    "word_counts1 = create_BoW(result1)\n",
    "word_counts2 = create_BoW(result2)\n",
    "print(\"BoW1:\")\n",
    "display(list(word_counts1.items())[:10])\n",
    "print(\"\\nBoW2:\")\n",
    "display(list(word_counts2.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec88e6",
   "metadata": {},
   "source": [
    "#### В случае с SpaCy (первым вариантом), скорее всего, происходит большая агрегация слов, и их частотность более высока, потому что лемматизация SpaCy может приводить к нормализации множества различных форм в одну лемму.\n",
    "#### В pymorphy3 (второй версии) лемматизация может быть немного менее агрессивной, оставляя больше уникальных вариантов для некоторых слов, что приводит к меньшему числу повторений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929e2c8",
   "metadata": {},
   "source": [
    "### <center>2-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96749799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-граммы1: [('алекс', 'каменев'), ('каменев', 'макс'), ('макс', 'вольф'), ('вольф', 'наёмник'), ('наёмник', 'глава'), ('глава', 'станция'), ('станция', 'технический'), ('технический', 'обслуживание'), ('обслуживание', 'лиманский'), ('лиманский', 'союз')]\n",
      "\n",
      "2-граммы2: [('алекс', 'каменеть'), ('каменеть', 'макс'), ('макс', 'вольф'), ('вольф', 'наёмник'), ('наёмник', 'глава'), ('глава', 'станция'), ('станция', 'технический'), ('технический', 'обслуживание'), ('обслуживание', 'бринг'), ('бринг', 'лиманский')]\n"
     ]
    }
   ],
   "source": [
    "bigrams1 = list(ngrams(result1, 2))\n",
    "print(f\"2-граммы1: {bigrams1[:10]}\")\n",
    "bigrams2 = list(ngrams(result2, 2))\n",
    "print(f\"\\n2-граммы2: {bigrams2[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695a7fc",
   "metadata": {},
   "source": [
    "### <center>4-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cd2b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-граммы1: [('алекс', 'каменев', 'макс', 'вольф'), ('каменев', 'макс', 'вольф', 'наёмник'), ('макс', 'вольф', 'наёмник', 'глава'), ('вольф', 'наёмник', 'глава', 'станция'), ('наёмник', 'глава', 'станция', 'технический'), ('глава', 'станция', 'технический', 'обслуживание'), ('станция', 'технический', 'обслуживание', 'лиманский'), ('технический', 'обслуживание', 'лиманский', 'союз'), ('обслуживание', 'лиманский', 'союз', 'приграничный'), ('лиманский', 'союз', 'приграничный', 'территория')]\n",
      "\n",
      "4-граммы2: [('алекс', 'каменеть', 'макс', 'вольф'), ('каменеть', 'макс', 'вольф', 'наёмник'), ('макс', 'вольф', 'наёмник', 'глава'), ('вольф', 'наёмник', 'глава', 'станция'), ('наёмник', 'глава', 'станция', 'технический'), ('глава', 'станция', 'технический', 'обслуживание'), ('станция', 'технический', 'обслуживание', 'бринг'), ('технический', 'обслуживание', 'бринг', 'лиманский'), ('обслуживание', 'бринг', 'лиманский', 'союз'), ('бринг', 'лиманский', 'союз', 'приграничный')]\n"
     ]
    }
   ],
   "source": [
    "fourgrams1 = list(ngrams(result1, 4))\n",
    "print(f\"4-граммы1: {fourgrams1[:10]}\")\n",
    "fourgrams2 = list(ngrams(result2, 4))\n",
    "print(f\"\\n4-граммы2: {fourgrams2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e250a546-0992-47bc-9b2f-48093dcbf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result_lab2/text_structures1.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"BoW\": word_counts1, \"bigrams\": bigrams1, \"fourgrams\": fourgrams1}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43048839-7f35-4057-9897-9a8202fba251",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result_lab2/text_structures2.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"BoW\": word_counts2, \"bigrams\": bigrams2, \"fourgrams\": fourgrams2}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
